{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463eb58c",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91165a",
   "metadata": {},
   "source": [
    "전시각 복습\n",
    "- 로지스틱회귀는 확률로 분류한다. (임계값 기준)\n",
    "\n",
    "기본 로지스틱 회귀의 결정경계는 선형(직선/초평면)이다.\\\n",
    "만약 데이터 경계가 곡선/복잡한 모양이면 기본 형태로는 잘 안 맞을 수 있다.\\\n",
    "(보완하려면 다항특성/비선형 변환 같은 추가 작업이 필요)\n",
    "\n",
    "이를 해결하기 위해 다양한 분류 방법이 존재하고,\\\n",
    "그중 하나가 오늘 배울 거리 기반 분류 방법 KNN이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706ac40",
   "metadata": {},
   "source": [
    "# KNN이란? - 거리 기반 다수결(이웃 투표)\n",
    "\"비슷한 애들끼리 같은 라벨일 거다\"라는 직관적인 가정으로 분류한다.\\\n",
    "새 데이터 x가 들어오면, 학습데이터에서 x와 가까운 K개 이웃을 찾고,\\\n",
    "그 이웃들의 라벨 다수결로 x의 라벨을 결정한다.\n",
    "\n",
    "> 모델을 먼저 학습해서 규칙을 만드는 게 아닌, 필요할 때마다 가까운 데이터 검색 → 투표로 예측하는 방식이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc347882",
   "metadata": {},
   "source": [
    "## K(이웃 수)의 의미 (중요)\n",
    "- K가 작다 (예: 1~3): 아주 국소적으로 판단 → 훈련 데이터에 딱 붙음\\\n",
    "과적합 위험 ↑ (*노이즈/이상치에 민감)\n",
    "\n",
    "- K가 크다 (예: 30, 50...): 더 넓게 평균내며 판단\\\n",
    "과소적합 위험 ↑ (세밀한 경계를 못 잡을 수 있음)\n",
    "\n",
    "- 때문에 가까운 k개를 찾아야 한다.\n",
    "\n",
    "> 팁: 동률 방지를 위해 K를 홀수로 시작하는 경우가 많고, 최종 K는 교차검증으로 고른다. (K는 하이퍼파라미터)\n",
    "\n",
    "### 예시 k=3\n",
    "새 데이터 *의 이웃 k(3)개:\n",
    "빨강 (거리 = 1.2)\\\n",
    "빨강 (거리 = 1.5)\\\n",
    "파랑 (거리 = 2.0)\n",
    "\n",
    "다수결: 빨강 2 > 파랑 1 → * = 삘강\n",
    "\n",
    "###### *노이즈: 데이터에 섞여 있는 의미 없는 흔들림/오차, 우연·측정오류·일시적 요인 때문에 생긴 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2824a21",
   "metadata": {},
   "source": [
    "## 가까운 이웃을 구하는 가장 기본 방식: 유클리드 거리\n",
    "유클리드 거리란? 두 점 사이의 \"직선 거리\"라고 보면 된다.\\\n",
    "\n",
    "직관 예시 (2차원 지도라고 생각)\n",
    "x가 내 위치.\\\n",
    "훈련 데이터가 \"주변 사람들의 위치\"\\\n",
    "유클리는 거리는 \"지도에서 자로 잰 직선 거리\" 느낌이다\n",
    "\n",
    "공식: \\\n",
    "\n",
    "\"가로로 얼마나 떨어졌나?\" 와 \"세로로 얼마나 떨어졌나?\"를 보고, 그것을 합쳐 전체 거리를 구하는 방식이다.\n",
    "\n",
    "> 핵심: **가로 차이 + 세로 차이를 합쳐서 '직선으로 얼마나 떨어졌는지'**를 본다.\n",
    "\n",
    "\n",
    "### 왜 표준화(스케일링)가 중요하냐? (진짜 중요)\n",
    "거리 계산은 숫자 크기에 민감하다.\n",
    "\n",
    "예를 들어\\\n",
    "키: 170 ~ 190 (차이 20)\\\n",
    "연봉: 3,000만 ~ 8,000만 (차이 5,000만)\n",
    "\n",
    "이렇게 섞여 있다면, 거리를 계산할 때 연봉 차이만 너무 크게 반영되어\\\n",
    "\"키가 비슷한 사람\"을 찾는것이 아닌 \"연봉이 비슷한 사람\"만 찾게 된다.\n",
    "\n",
    "**그래서 KNN은 보통 표준화/정규화가 거의 필수다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68742847",
   "metadata": {},
   "source": [
    "### 나머지 거리 방법들\n",
    "1) 맨해튼 거리(Manhattan)\\\n",
    "직선이 아니라 가로+세로로만 이동한다고 생각하면 된다. (도시 블록에서 길 따라 걷는 느낌)\n",
    "\n",
    "2) 코사인 유사도(Cosine)\\\n",
    "\"거리\"보다 방향이 비슷한지 본다.\\\n",
    "특히 텍스트나 임베딩처럼 **벡터 방향**이 중요할 때 자주 쓴다.\n",
    "\n",
    "3) 해밍 거리(Hamming)\\\n",
    "0/1 같은 데이터에서 서로 다른 칸이 몇 개인지 세는 방식이다.\\\n",
    "(예: 10101 vs 11100 → 다른 자리 개수 세기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76653f6d",
   "metadata": {},
   "source": [
    "### 정리 한 줄\n",
    "- 유클리드 = 직선 거리(기본)\n",
    "- 맨해튼 = 가로+세로 거리\n",
    "- 코사인 = 방향 유사도\n",
    "- 해밍 = 다른 자리 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7facf",
   "metadata": {},
   "source": [
    "## KNN이 분류하는 흐름\n",
    "1. 거리 정의\n",
    "    - 보통 연속형 특성에서는 유클리드 거리 사용\n",
    "    - 다른 거리도 가능(맨해튼, 코사인 등)\n",
    "2. 가까운 K개 이웃 찾기\n",
    "    새 샘플 x에 대해 훈련 데이터 전체와의 거리를 계산하고 가까운 K개 선택\n",
    "3. 다수결(또는 거리 가중치)로 예측\n",
    "    - 기본: K개 중 가장 많은 라벨\n",
    "    - 변형: 더 가까운 이웃에 가중치를 더 준다(distance-weighted)\n",
    "\n",
    "> KNN은 학습보단, 필요할 때마다 \"검색해 투표\"하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fabc3",
   "metadata": {},
   "source": [
    "## KNN의 장단점\n",
    "- 장점\n",
    "    - 개념이 단순하고 직관적\n",
    "    - 복잡한 결정경계(휘어진 경계)도 잘 잡을 수 있다.\n",
    "\n",
    "- 단점\n",
    "    - 예측할 때마다 거리 계산이 필요 → 데이터가 크면 느림\n",
    "    - 학습 데이터를 거의 그대로 저장 → 메모리 부담 가능\n",
    "    - 스케일(단위)에 매우 민감하다.\n",
    "        - 예: 키(cm)와 연봉(원)을 같이 쓰면 연봉이 거리를 지배해버림\n",
    "        - 그래서 보통 표준화/정규화가 거의 필수\n",
    "- 차원이 커질수록 거리의 의미가 약해짐 → 성능이 떨어질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09816de5",
   "metadata": {},
   "source": [
    "## KNN에서 스케일링이 왜 중요할까?\n",
    "KNN은 새 데이터 x가 들어오면, 훈련 데이터들과의 거리를 계산하고\n",
    "가장 가까운 K개를 뽑아 그 라벨을 다수결로 결정한다.\n",
    "\n",
    "즉, 거리 계산이 곧 모델의 판단 기준이다. \\\n",
    "그래서 거리 계산이 왜곡되면 KNN의 결과도 바로 망가져버리기에 스케일링이 사실상 전제다.\n",
    "\n",
    "### 스케일(단위)이 다르면 \"큰 단위\"가 거리 계산을 지배한다\n",
    "특성(변수)들이 단위가 다르면, 값이 큰 변수 쪽이 거리 계산에 훨씬 크게 영향을 준다.\\\n",
    "즉, 의도와 다르게 한 변수만 보고 분류하는 모델이 되어버릴 수 있다.\n",
    "\n",
    "- 쉬운 예시\n",
    "    - 키(cm): 보통 150~190 정도\\\n",
    "    - 연봉(원): 보통 30,000,000~80,000,000 정도\\\n",
    "    - 이 상태로 거리를 계산하면, 키 차이는 10cm는 '10', 연봉 차이 1,000만 원은 '10,000,000' 차이\n",
    "- 거리 계산에서 연봉 차이가 너무 커 사실상 KNN은 \"키가 비슷한 이웃\"이 아닌 \"연봉이 비슷한 이웃\"만 찾게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe72d4",
   "metadata": {},
   "source": [
    "### 스케일링을 하면 \"모든 변수의 영향력을 비슷하게 맞춘다\"\n",
    "스케일링은 각 변수를 비슷한 크기 범위로 맞춰서, 거리 계산이 특정 변수에만 끌려가지 않게 한다.\n",
    "\n",
    "대표적인 스케일링 2가지 (KNN에서 자주 씀)\n",
    "1) 표준화(Standardization)\\\n",
    "평균을 0으로, 표준편차를 1로 맞춤\\\n",
    "\"평균에서 몇 표준편차 떨어졌나?\" 기준으로 비교\\\n",
    "-특징\\\n",
    "1.이상치가 많으면 영향이 있을 수 있으나 실무에서 제일 흔하게 사용\n",
    "\n",
    "2) 정규화(Min-Max Normalization)\\\n",
    "값을 0~1 사이로 맞춤\\\n",
    "-특징\\\n",
    "1.값의 범위를 깔끔하게 맞추고 싶을 때 유용\\\n",
    "2.이상치가 있으면 전체가 눌릴 수 있음\n",
    "\n",
    "### 실전에서 꼭 기억할 2줄 팁.\n",
    "- 스케일러는 train으로만 fit하고, val/test에는 transform만 한다. (데이터 누수 방지)\n",
    "- 보통은 **표준화(Standardization)**를 기본으로 쓰고, 이상치가 심하면 다른 스케일러(예: Robust)를 고민한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5399cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982622c",
   "metadata": {},
   "source": [
    "# 디시전 바운더리\n",
    "분류 모델은 입력  x를 보고 **A냐 B냐(0이냐 1이냐)**를 결정한다.\\\n",
    "이때 A로 판단하는 영역과 B로 판단하는 영역이 생기는데 그 **경계선(또는 경계면)**을 디시전 바운더리라 부른다.\n",
    "\n",
    "> 클래스가 바뀌는 경계\n",
    "\n",
    "\n",
    "## 2차원으로 생각하면 쉽게 접근 가능하다.\n",
    "특성이 2개라고 하자. (예: 공부시간, 모의고사 점수)\n",
    "\n",
    "- 점 하나 = 사람 한 명(데이터 한 개)\n",
    "- 색 = 라벨(합격/불합격)\n",
    "\n",
    "이때 모델을 분류하면 평면은 이렇게 나눠진다.\\\n",
    "- 왼쪽 영역: 모델이 불합격(0)\n",
    "- 오른쪽 영역: 모델이 합격(1)\n",
    "\n",
    "이때 0 영역과 1 영역이 갈리는 선이 바로 결정경계, 디시전 바운더리다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcbaf9",
   "metadata": {},
   "source": [
    "## 모델별로 결정경계 모양이 달라진다\n",
    "- 로지스틱 회귀(기본형)\\\n",
    "결정경계가 보통 직선(또는 평평한 면) 느낌\\\n",
    "그래서 데이터 경계가 휘어져 있으면 한 번에 잘 못 자를 수 있음\n",
    "\n",
    "- KNN\\\n",
    "이웃을 따라가니까 결정경계가 울퉁불퉁해질 수 있음\\\n",
    "K가 작으면 더 울퉁불퉁(복잡), K가 크면 더 매끈(단순)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78775ca",
   "metadata": {},
   "source": [
    "## 디시전 바운더리가 중요한 이유: 과적합과 과소적합\n",
    "- 경계가 너무 구불구불하고 점들을 피해 다니면 → 과적합 가능성 ↑\n",
    "    - 모델이 너무 단순해서 패턴을 제대로 못 잡는 상태. 결정경계가 너무 단순하다. (대충 직선 한 줄로 끝)\n",
    "    - 훈련 데이터도 잘 못 맞추고, 테스트 데이터도 못 맞춘다.\n",
    "        - train/test 점수가 낮음\n",
    "\n",
    "- 경계가 너무 단순해서 점들을 많이 잘라버리면 → 과소적합 가능성 ↑\n",
    "    - 모델이 너무 복잡해서 훈련 데이터의 \"노이즈\"까지 외워버린 상태. 결정경계가 구불구불, 점 하나하나 피해 다니는 느낌\n",
    "    - 훈련 데이터는 엄청 잘 맞추나 새 데이터(테스트)엔 잘 안 맞음\n",
    "        - train 점수 높으나 test 점수 낮음 (차이가 큼)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0aa15",
   "metadata": {},
   "source": [
    "# gridsearch cv로 최적의 k 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5464b88",
   "metadata": {},
   "source": [
    "# 차원의 저주 - knn은 왜 고차원에서 약해질까?\n",
    "차원이 늘수록(특성 수가 많을수록) \"가까움\"과 \"멀음\"의 차이가 희미해진다.\\\n",
    "그러면 KNN이 \"진짜 이웃\"을 찾기 어려워진다.\n",
    "\n",
    "> KNN의 핵심은 거리.\\\n",
    "고차원에서는 거리들이 비슷해지고, 데이터가 희박해져 \"가까운 이웃\"이 더 이상 신뢰할 만한 기준이 아니게 된다.\n",
    "\n",
    "## 개념 설명 (KNN 관점)\n",
    "KNN은 모든 샘플과의 거리를 재고 가장 가까운 K개를 골라 다수결로 분류한다.\\\n",
    "만약 데이터가 고차원이 될 경우...\n",
    "\n",
    "### 거리들이 다 비슷해진다.: \"가장 가까운 이웃\"이 사실상 그렇게 가깝지 않게 된다.\n",
    "차원이 많아지면, 한 변수에서 조금 가까워도 다른 변수들에서 조금씩 멀어질 확률이 커진다.\\\n",
    "이 \"조금씩\"이 여러 차원에 쌓여, 결과적으로 \n",
    "- 제일 가까운 점도\n",
    "- 그다음 가까운 점도\n",
    "- 꽤 멀어 보이는 점도\n",
    "\n",
    "거리 차이가 크게 안 나게 된다.\n",
    "\n",
    "### 데이터가 희박(sparse)해진다. : 다수결이 의미가 약해진다.\n",
    "차원이 늘어날수록 공간의 \"부피\"가 엄청 커진다.\\\n",
    "그런데 데이터 개수는 보통 그대로니까, 점들이 공간에 드문드문 흩어져 있게 된다.\n",
    "그래서\\\n",
    "내 주변에 진짜 가까운 이웃이 거의 없고\\\n",
    "이웃을 찾으려면 반경을 크게 키워야 하는데\\\n",
    "그러면 이웃이 \"비슷한 애들\"이 아닌 \"그냥 아무나\"가 된다.\n",
    "\n",
    "\n",
    "### 간단 예시(감각만)\n",
    "특성 2개(2차원)면 \"비슷한 점\"이 주변에 모여있을 가능성이 크다.\\\n",
    "특성 50개(50차원)면 \"모든 특성이 동시에 비슷한 점\"을 찾기가 어렵다.\n",
    "\n",
    "즉, \"키도 비슷하고, 몸무게도 비슷하고, 나이도 비슷하고, … 50개가 다 비슷한 사람\"을 찾는 상황이 되어버린다.\\\n",
    "현실적으로도, 데이터 적으로도 그런 이웃을 찾기 어렵다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
